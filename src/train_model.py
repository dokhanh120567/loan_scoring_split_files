import os
import sys
import joblib
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# âœ… Add PYTHONPATH náº¿u cháº¡y ngoÃ i Docker
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.preprocess import load_data, fit_transformers, preprocess_for_training

# ----- ÄÆ°á»ng dáº«n lÆ°u mÃ´ hÃ¬nh -----
MODEL_PATH = "model/xgb_model.json"
ENCODER_PATH = "model/ohe.pkl"
SCALER_PATH = "model/scaler.pkl"
FEATURES_PATH = "model/feature_cols.pkl"

def train():
    # Load data
    df = load_data()
    print(f"âœ… Dá»¯ liá»‡u Ä‘á»c tá»« DB: {df.shape}")
    
    # Fit transformers
    ohe, scaler = fit_transformers(df)
    
    # Preprocess data
    X, y, feature_names = preprocess_for_training(df, ohe, scaler)
    
    # Save feature names
    joblib.dump(feature_names, FEATURES_PATH)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    model = train_model(X_train, y_train)
    
    # Evaluate model
    evaluate_model(model, X_test, y_test)
    
    # Save model and transformers
    save_model(model, ohe, scaler)
    
    print("âœ… Training completed successfully!")

def train_model(X_train, y_train):
    # ğŸ‘‰ Chuyá»ƒn sang DMatrix
    dtrain = xgb.DMatrix(X_train, label=y_train)

    # ğŸ‘‰ Cáº¥u hÃ¬nh XGBoost vá»›i cÃ¡c tham sá»‘ Ä‘Æ°á»£c tinh chá»‰nh
    params = {
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "max_depth": 4,  # Giáº£m Ä‘á»™ sÃ¢u Ä‘á»ƒ trÃ¡nh overfitting
        "eta": 0.05,     # Giáº£m learning rate Ä‘á»ƒ há»c cháº­m hÆ¡n
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "min_child_weight": 3,  # TÄƒng Ä‘á»ƒ trÃ¡nh overfitting
        "gamma": 0.2,    # TÄƒng Ä‘á»ƒ tÄƒng Ä‘á»™ cháº·t cháº½ cá»§a cÃ¢y
        "tree_method": "hist",
        "scale_pos_weight": 1.5,  # Äiá»u chá»‰nh cho class imbalance
        "max_leaves": 16,  # Giá»›i háº¡n sá»‘ lÃ¡ Ä‘á»ƒ trÃ¡nh overfitting
        "max_bin": 256,   # TÄƒng Ä‘á»™ chÃ­nh xÃ¡c cá»§a histogram
        "grow_policy": "lossguide"  # Táº­p trung vÃ o viá»‡c giáº£m loss
    }

    # ğŸ‘‰ Train model vá»›i early stopping
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=500,  # TÄƒng sá»‘ vÃ²ng láº·p
        evals=[(dtrain, "train")],
        early_stopping_rounds=50,  # TÄƒng sá»‘ vÃ²ng dá»«ng sá»›m
        verbose_eval=10
    )
    return model

def evaluate_model(model, X_test, y_test):
    # ğŸ‘‰ TÃ­nh AUC
    y_pred = model.predict(xgb.DMatrix(X_test))
    auc = roc_auc_score(y_test, y_pred)
    print(f"ğŸ¯ Test AUC: {auc:.4f}")

def save_model(model, ohe, scaler):
    # ğŸ‘‰ LÆ°u mÃ´ hÃ¬nh vÃ  transformer
    os.makedirs("model", exist_ok=True)
    model.save_model(MODEL_PATH)
    joblib.dump(ohe, ENCODER_PATH)
    joblib.dump(scaler, SCALER_PATH)
    print(f"\nâœ… MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {MODEL_PATH}")
    print(f"âœ… Encoder & Scaler saved vÃ o thÆ° má»¥c model/")

if __name__ == "__main__":
    train()
